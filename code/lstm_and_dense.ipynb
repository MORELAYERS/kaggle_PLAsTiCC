{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import lightgbm as lgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, BatchNormalization, Dropout\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_curves = pd.read_csv('../data/raw/training_set.csv')\n",
    "print(\"train_curves columns: {}\".format(train_curves.columns))\n",
    "print(\"train_passbands shape: {}\".format(train_curves.shape))\n",
    "train_curves.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv('../data/raw/training_set_metadata.csv')\n",
    "print(\"train_metadata columns: {}\".format(train_metadata.columns))\n",
    "print(\"train_metadata shape: {}\".format(train_metadata.shape))\n",
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorganize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge curves and metadata into full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = train_curves.reset_index(drop=True).merge(\n",
    "    right=train_metadata,\n",
    "    how='outer',\n",
    "    on='object_id'\n",
    ")\n",
    "print(\"full_train columns: {}\".format(full_train.columns))\n",
    "print(\"full_train shape: {}\".format(full_train.shape))\n",
    "full_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable() # включение garbage collector\n",
    "\n",
    "X_metadata = train_curves.copy() # считывание тренировочных данных (кривые)\n",
    "X_metadata['flux_ratio_sq'] = np.power(X_metadata['flux'] / X_metadata['flux_err'], 2.0)\n",
    "X_metadata['flux_by_flux_ratio_sq'] = X_metadata['flux'] * X_metadata['flux_ratio_sq']\n",
    "\n",
    "aggs = {\n",
    "    'mjd': ['min', 'max', 'size'],\n",
    "    'passband': ['min', 'max', 'mean', 'median', 'std'],\n",
    "    'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "    'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "    'detected': ['mean'],\n",
    "    'flux_ratio_sq':['sum','skew'],\n",
    "    'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "}\n",
    "\n",
    "X_metadata = X_metadata.groupby('object_id').agg(aggs)\n",
    "new_columns = [\n",
    "    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "]\n",
    "X_metadata.columns = new_columns\n",
    "X_metadata['mjd_diff'] = X_metadata['mjd_max'] - X_metadata['mjd_min']\n",
    "X_metadata['flux_diff'] = X_metadata['flux_max'] - X_metadata['flux_min']\n",
    "X_metadata['flux_dif2'] = (X_metadata['flux_max'] - X_metadata['flux_min']) / X_metadata['flux_mean']\n",
    "X_metadata['flux_w_mean'] = X_metadata['flux_by_flux_ratio_sq_sum'] / X_metadata['flux_ratio_sq_sum']\n",
    "X_metadata['flux_dif3'] = (X_metadata['flux_max'] - X_metadata['flux_min']) / X_metadata['flux_w_mean']\n",
    "\n",
    "del X_metadata['mjd_max'], X_metadata['mjd_min']\n",
    "gc.collect() # сбор мусора\n",
    "\n",
    "print(\"X_metadata columns: {}\".format(X_metadata.columns))\n",
    "print(\"X_metadata shape: {}\".format(X_metadata.shape))\n",
    "X_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_metadata = X_metadata.reset_index().merge(\n",
    "    right=train_metadata,\n",
    "    how='outer',\n",
    "    on='object_id'\n",
    ")\n",
    "\n",
    "if 'target' in X_metadata:\n",
    "    y = X_metadata['target']\n",
    "    del X_metadata['target']\n",
    "classes = sorted(y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'object_id' in X_metadata:\n",
    "    oof_df = X_metadata[['object_id']]\n",
    "    del X_metadata['object_id'], X_metadata['distmod'], X_metadata['hostgal_specz'] # удаление колонок\n",
    "    del X_metadata['ra'], X_metadata['decl'], X_metadata['gal_l'], X_metadata['gal_b'], X_metadata['ddf'] # удаление колонок\n",
    "    \n",
    "print(\"X_metadata columns: {}\".format(X_metadata.columns))\n",
    "print(\"X_metadata shape: {}\".format(X_metadata.shape))\n",
    "X_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_metadata_ss = ss.fit_transform(X_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for name, group in full_train.groupby('object_id'):\n",
    "    y.append(group['target'].iloc[0])\n",
    "    \n",
    "y = np.array(y)\n",
    "y_classes = np.array(y)\n",
    "y_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y)\n",
    "print(\"y shape: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = full_train[['object_id', 'passband', 'mjd', 'flux', 'flux_err', 'target']]\n",
    "print(\"X_temp columns: {}\".format(X_temp.columns))\n",
    "print(\"X_temp shape: {}\".format(X_temp.shape))\n",
    "X_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_curve = [[],[],[],[],[],[], [], [], [], [], [], []]\n",
    "for name, group in X_temp.groupby(['object_id', 'passband']):\n",
    "    X_curve[name[1]].append(group['flux'])\n",
    "    X_curve[name[1] + 6].append(group['flux_err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_curve = np.array([sequence.pad_sequences(x, maxlen=maxlen, dtype='float32', padding='post') for x in X_curve]).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_curve shape: {}\".format(X_curve.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69795\n",
    "def mywloss(y_true,y_pred):  \n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function for poster evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_ohe, y_p):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weight table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Костыль\n",
    "wtable = np.array([0.01924057, 0.06307339, 0.117737  , 0.15201325, 0.02331804,\n",
    "       0.00382263, 0.06167176, 0.01299694, 0.125     , 0.02650357,\n",
    "       0.04714577, 0.29472477, 0.03045362, 0.02229867])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 200\n",
    "n_epoch = 50\n",
    "n_features = 12\n",
    "n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1) # деление данных на фолды для кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = np.unique(y_classes)\n",
    "class_map = dict()\n",
    "for i,val in enumerate(unique_y):\n",
    "    class_map[val] = i\n",
    "\n",
    "print(unique_y)\n",
    "print()\n",
    "print(class_map)\n",
    "        \n",
    "y_map = np.zeros((y_classes.shape[0],))\n",
    "y_map = np.array([class_map[val] for val in y_classes])\n",
    "y_categorical = to_categorical(y_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Dense on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def build_model(dropout_rate=0.25):\n",
    "    \n",
    "    curve_input = Input(shape=(None, n_features), name='curve_input')\n",
    "    lstm_hidden = LSTM(256, return_sequences=True, dropout=0.1, name='lstm_1')(curve_input)\n",
    "    lstm_out = LSTM(64, dropout=0.1, name='lstm_2')(lstm_hidden)\n",
    "    \n",
    "    metadata_input = Input(shape=(31,), name='metadata_input')\n",
    "    x = keras.layers.concatenate([lstm_out, metadata_input], axis=1)\n",
    "    \n",
    "    dense_1 = Dense(512, activation='relu')(x)\n",
    "    dense_1 = BatchNormalization()(dense_1)\n",
    "    dense_1 = Dropout(dropout_rate)(dense_1)\n",
    "    \n",
    "    dense_2 = Dense(256, activation='relu')(dense_1)\n",
    "    dense_2 = BatchNormalization()(dense_2)\n",
    "    dense_2 = Dropout(dropout_rate)(dense_2)\n",
    "    \n",
    "    dense_3 = Dense(128, activation='relu')(dense_2)\n",
    "    dense_3 = BatchNormalization()(dense_3)\n",
    "    dense_3 = Dropout(dropout_rate)(dense_3)\n",
    "    \n",
    "    dense_4 = Dense(128, activation='relu')(dense_3)\n",
    "    dense_4 = BatchNormalization()(dense_4)\n",
    "    dense_4 = Dropout(dropout_rate)(dense_4)\n",
    "    \n",
    "    output = Dense(n_classes, activation='softmax', name='output')(dense_4)\n",
    "\n",
    "    model = Model(inputs=[curve_input, metadata_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and Dense with Dense on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def build_model_equal(dropout_rate=0.25):\n",
    "    \n",
    "    curve_input = Input(shape=(None, n_features), name='curve_input')\n",
    "    \n",
    "    lstm_hidden_1 = LSTM(256, return_sequences=True, dropout=0.1, name='lstm_1')(curve_input)\n",
    "    lstm_hidden_2 = LSTM(64, dropout=0.1, name='lstm_2')(lstm_hidden_1)\n",
    "    \n",
    "    lstm_out = Dense(32)(lstm_hidden_2)\n",
    "    \n",
    "    metadata_input = Input(shape=(31,), name='metadata_input')\n",
    "    \n",
    "    dense_1 = Dense(512, activation='relu')(metadata_input)\n",
    "    dense_1 = BatchNormalization()(dense_1)\n",
    "    dense_1 = Dropout(dropout_rate)(dense_1)\n",
    "    \n",
    "    dense_2 = Dense(256, activation='relu')(dense_1)\n",
    "    dense_2 = BatchNormalization()(dense_2)\n",
    "    dense_2 = Dropout(dropout_rate)(dense_2)\n",
    "    \n",
    "    dense_3 = Dense(128, activation='relu')(dense_2)\n",
    "    dense_3 = BatchNormalization()(dense_3)\n",
    "    dense_3 = Dropout(dropout_rate)(dense_3)\n",
    "    \n",
    "    dense_4 = Dense(128, activation='relu')(dense_3)\n",
    "    dense_4 = BatchNormalization()(dense_4)\n",
    "    dense_4 = Dropout(dropout_rate)(dense_4)\n",
    "    \n",
    "    dense_out = Dense(32)(dense_4)\n",
    "    \n",
    "    x = keras.layers.concatenate([lstm_out, dense_out], axis=1)\n",
    "    \n",
    "    output_hidden = Dense(64)(x)\n",
    "    output_hidden = BatchNormalization()(output_hidden)\n",
    "    output_hidden = Dropout(dropout_rate)(output_hidden)\n",
    "    \n",
    "    output = Dense(n_classes, activation='softmax', name='output')(output_hidden)\n",
    "\n",
    "    model = Model(inputs=[curve_input, metadata_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "oof_preds = np.zeros((len(X_metadata_ss), n_classes))\n",
    "checkPoint = ModelCheckpoint(\"./keras_lstm.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "    x_train, x_metadata, y_train = X_curve[trn_], X_metadata_ss[trn_], y.iloc[trn_]\n",
    "    x_valid, x_metadata_val, y_valid = X_curve[val_], X_metadata_ss[val_], y.iloc[val_]\n",
    "    \n",
    "    model = build_model_equal()\n",
    "    model.compile(loss=mywloss, optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit([x_train, x_metadata], y_train,\n",
    "                    validation_data=[[x_valid, x_metadata_val], y_valid], \n",
    "                    epochs=n_epoch,\n",
    "                    batch_size=n_batch, shuffle=True, verbose=2, callbacks=[checkPoint])  \n",
    "    \n",
    "    plot_loss_acc(history)\n",
    "    \n",
    "    print('Loading Best Model')\n",
    "    model.load_weights('./keras_lstm.model')\n",
    "    # # Get predicted probabilities for each class\n",
    "    oof_preds[val_, :] = model.predict([x_valid, x_metadata_val],batch_size=n_batch)\n",
    "    print(multi_weighted_logloss(y_valid, model.predict([x_valid, x_metadata_val], batch_size=n_batch)))\n",
    "    clfs.append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "class_names = list(sample_sub.columns[1:-1])\n",
    "del sample_sub;gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(12,12))\n",
    "foo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test = pd.read_csv('../data/raw/test_set_metadata.csv') # считывание тестовых данных (кривые)\n",
    "\n",
    "import time\n",
    "\n",
    "temp_shit = 1\n",
    "\n",
    "start = time.time()\n",
    "chunks = 15000000\n",
    "X_prev = np.array([[],[],[],[],[],[],[],[],[],[],[],[]])\n",
    "for i_c, df in enumerate(pd.read_csv('../data/raw/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(\"Creating meta\")\n",
    "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
    "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
    "    # Group by object id\n",
    "    agg_test = df.groupby('object_id').agg(aggs)\n",
    "    agg_test.columns = new_columns\n",
    "    agg_test['mjd_diff'] = agg_test['mjd_max'] - agg_test['mjd_min']\n",
    "    agg_test['flux_diff'] = agg_test['flux_max'] - agg_test['flux_min']\n",
    "    agg_test['flux_dif2'] = (agg_test['flux_max'] - agg_test['flux_min']) / agg_test['flux_mean']\n",
    "    agg_test['flux_w_mean'] = agg_test['flux_by_flux_ratio_sq_sum'] / agg_test['flux_ratio_sq_sum']\n",
    "    agg_test['flux_dif3'] = (agg_test['flux_max'] - agg_test['flux_min']) / agg_test['flux_w_mean']\n",
    "\n",
    "    del agg_test['mjd_max'], agg_test['mjd_min']\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "    \n",
    "    # Merge with meta data\n",
    "    full_test = agg_test.reset_index().merge(\n",
    "        right=meta_test,\n",
    "        how='left',\n",
    "        on='object_id'\n",
    "    )\n",
    "    full_test[full_train_new.columns] = full_test[full_train_new.columns].fillna(full_train_new.mean(axis=0))\n",
    "    full_test_ss = ss.transform(full_test[full_train_new.columns])\n",
    "    \n",
    "    print(\"Creating curve\")\n",
    "    X_curve = df[['object_id', 'passband', 'mjd', 'flux', 'flux_err']]\n",
    "    \n",
    "#     print(type(X_prev))\n",
    "    if isinstance(X_prev, (np.ndarray, np.generic)):\n",
    "#         print(\"YES I AM A LIST AND \", type(X_prev))\n",
    "        X_prev = X_prev.tolist()\n",
    "    if isinstance(X_prev[0], (np.ndarray, np.generic)):\n",
    "        for i in range(len(X_prev)):\n",
    "            if isinstance(X_prev[i], (np.ndarray, np.generic)):\n",
    "                X_prev[i] = X_prev[i].tolist()\n",
    "    X_new = X_prev\n",
    "#     print(X_prev[0])\n",
    "    X_prev = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "    for name, group in X_curve.groupby(['object_id', 'passband']):\n",
    "#         print(name, \": \",group.shape)\n",
    "        if len(group['flux']) != 0: \n",
    "            X_new[name[1]].append(group['flux'])\n",
    "        else:\n",
    "            X_new[name[1]].append([0])\n",
    "#             print(\"asdfasdfasdf\")\n",
    "        if len(group['flux']) != 0: \n",
    "            X_new[name[1] + 6].append(group['flux_err'])\n",
    "        else:\n",
    "            X_new[name[1] + 6].append([0])\n",
    "    \n",
    "    print(\"Padding\")\n",
    "    \n",
    "    for i, x in enumerate(X_new):\n",
    "#         print(\"old: {}\".format(len(x[0])))\n",
    "        temp = sequence.pad_sequences(x, maxlen=maxlen, dtype='float32', padding='post')\n",
    "#         print(\"new: {}\".format(len(temp[0])))\n",
    "        X_new[i] = temp\n",
    "    \n",
    "    for _ in range(20):\n",
    "        for i in range(6):\n",
    "            if len(X_new[i]) != min([len(x) for x in X_new]):\n",
    "    #             print(\"STORING VALUES...\")\n",
    "                X_prev[i].append(X_new[i][-1])\n",
    "                X_new[i] = np.delete(X_new[i], (-1), axis=0)\n",
    "\n",
    "            if len(X_new[i+6]) != min([len(x) for x in X_new]):\n",
    "    #             print(\"STORING VALUES...\")\n",
    "                X_prev[i+6].append(X_new[i+6][-1])\n",
    "                X_new[i+6] = np.delete(X_new[i+6], (-1), axis=0)\n",
    "        \n",
    "    X_copy = np.array(X_new)\n",
    "    print(X_copy.shape)\n",
    "    for x in X_copy:\n",
    "        print(\"shape: \", x.shape)\n",
    "        \n",
    "    X_copy = X_copy.transpose(1, 2, 0)\n",
    "    #X_copy = np.array([sequence.pad_sequences(x, maxlen=maxlen, dtype='float32', padding='post') for x in X_new]).transpose(1, 2, 0)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions\")\n",
    "    preds = None\n",
    "    for clf in clfs:\n",
    "        print(\"classifier...\")\n",
    "        if preds is None:\n",
    "            preds = clf.predict([X_copy, full_test_ss]) / folds.n_splits\n",
    "        else:\n",
    "            preds += clf.predict([X_copy, full_test_ss]) / folds.n_splits\n",
    "    \n",
    "   # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds.shape[0])\n",
    "    for i in range(preds.shape[1]):\n",
    "        preds_99 *= (1 - preds[:, i])\n",
    "    \n",
    "    # Store predictions\n",
    "    preds_df = pd.DataFrame(preds, columns=class_names)\n",
    "    preds_df['object_id'] = full_test['object_id']\n",
    "    preds_df['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "    \n",
    "    print(\"Writing to CSV\")\n",
    "    if i_c == 0:\n",
    "        preds_df.to_csv('../data/submissions/predictions_lstm.csv',  header=True, mode='a', index=False)\n",
    "    else: \n",
    "        preds_df.to_csv('../data/submissions/predictions_lstm.csv',  header=False, mode='a', index=False)\n",
    "        \n",
    "    del agg_test, full_test, preds_df, preds\n",
    "#     print('done')\n",
    "    if (i_c + 1) % 10 == 0:\n",
    "        print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
